{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xmXVYxM8Ne98",
        "outputId": "9a5502b6-3f8f-45fc-eb51-03d99016c1a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.5.3 (from -r requirements.txt (line 1))\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 2))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting opendatasets==0.1.22 (from -r requirements.txt (line 3))\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting Pillow==9.4.0 (from -r requirements.txt (line 4))\n",
            "  Downloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting torch==2.0.0 (from -r requirements.txt (line 5))\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.5.1)\n",
            "Collecting torchvision==0.15.1 (from -r requirements.txt (line 7))\n",
            "  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting tqdm==4.65.0 (from -r requirements.txt (line 8))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets==0.1.22->-r requirements.txt (line 3)) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets==0.1.22->-r requirements.txt (line 3)) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 5)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 5)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 5)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 5)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 5)) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1->-r requirements.txt (line 7)) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 5)) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 5)) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 5)) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 5))\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->-r requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets==0.1.22->-r requirements.txt (line 3)) (2024.8.30)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets==0.1.22->-r requirements.txt (line 3)) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets==0.1.22->-r requirements.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets==0.1.22->-r requirements.txt (line 3)) (6.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1->-r requirements.txt (line 7)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets==0.1.22->-r requirements.txt (line 3)) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets==0.1.22->-r requirements.txt (line 3)) (1.3)\n",
            "Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Downloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, tqdm, Pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, opendatasets, triton, torch, torchvision\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.6\n",
            "    Uninstalling tqdm-4.66.6:\n",
            "      Successfully uninstalled tqdm-4.66.6\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.3 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-9.4.0 lit-18.1.8 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 opendatasets-0.1.22 pandas-1.5.3 torch-2.0.0 torchvision-0.15.1 tqdm-4.65.0 triton-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              },
              "id": "f6dd0914afce41ebacf315a6a7618d61"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data"
      ],
      "metadata": {
        "id": "FW5S1x95dA83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import glob\n",
        "import pickle\n",
        "import os"
      ],
      "metadata": {
        "id": "l01TXX4kNe2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_images_list = glob.glob(f\"/content/drive/MyDrive/images_001/images/*.png\", recursive=True)\n",
        "len(all_images_list)\n",
        "\n",
        "# Shuffle the data if the list is not empty\n",
        "if all_images_list:\n",
        "    random.shuffle(all_images_list)\n",
        "else:\n",
        "    print(\"No images found. Please check the file path or directory structure.\")\n",
        "\n",
        "# Print the first 10 image paths if available\n",
        "print(all_images_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_YTBLHVNe0q",
        "outputId": "b0c860e8-be21-4f1c-c025-e621d01f7922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/images_001/images/00000877_037.png', '/content/drive/MyDrive/images_001/images/00000193_013.png', '/content/drive/MyDrive/images_001/images/00000632_012.png', '/content/drive/MyDrive/images_001/images/00001219_000.png', '/content/drive/MyDrive/images_001/images/00000508_000.png', '/content/drive/MyDrive/images_001/images/00000963_008.png', '/content/drive/MyDrive/images_001/images/00000368_003.png', '/content/drive/MyDrive/images_001/images/00000248_001.png', '/content/drive/MyDrive/images_001/images/00000591_004.png', '/content/drive/MyDrive/images_001/images/00001104_018.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Spliting the data into train and test\n",
        "train_images = all_images_list[:4500]\n",
        "test_images = all_images_list[4500:]"
      ],
      "metadata": {
        "id": "cE3nCvTwNeyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "PetKoyBodE5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "from prepare_data import TrainDataset, ValDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from model_architecture import Generator, Discriminator\n",
        "from custom_loss import Generator_Loss\n",
        "from model_metrics import ssim\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.cuda.manual_seed_all(42)"
      ],
      "metadata": {
        "id": "QWPJslzTiKOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize parameters\n",
        "CROP_SIZE = 100\n",
        "UPSCALE_FACTOR = 4\n",
        "NUM_EPOCHS = 8\n",
        "BATCH_SIZE = 2\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "VbXzG4kMjuI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize the generator\n",
        "netG = Generator(upscale_factor=4).to(DEVICE)\n",
        "print(\"# generator parameters:\", sum(param.numel() for param in netG.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k5nvFN5l66g",
        "outputId": "d5faca31-ed1a-4aaf-c6ed-4ee0c69f2cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# generator parameters: 201209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize the discriminator\n",
        "netD = Discriminator().to(DEVICE)\n",
        "print(\"# discriminator parameters:\", sum(param.numel() for param in netD.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR0YNxETl8ut",
        "outputId": "9c774fbb-28f8-4454-f281-24b15fee3b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# discriminator parameters: 19413279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize the loss function\n",
        "generator_criterion = Generator_Loss().to(DEVICE)"
      ],
      "metadata": {
        "id": "r-oERNf0l-Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize the optimizer\n",
        "optimizerG = torch.optim.AdamW(netG.parameters(), lr=1e-3)\n",
        "optimizerD = torch.optim.AdamW(netD.parameters(), lr=1e-3)\n",
        "\n",
        "## Initialize the dictionary to store the results\n",
        "results = {\n",
        "    \"d_loss\": [],\n",
        "    \"g_loss\": [],\n",
        "    \"d_score\": [],\n",
        "    \"g_score\": [],\n",
        "    \"psnr\": [],\n",
        "    \"ssim\": [],\n",
        "}"
      ],
      "metadata": {
        "id": "Z9qGo9yvl_dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the train dataset\n",
        "print(\"[INFO] Loading Train dataset\")\n",
        "train_set = TrainDataset(train_images)\n",
        "\n",
        "## Load the validation dataset\n",
        "print(\"[INFO] Loading Val dataset\")\n",
        "val_set = ValDataset(test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3j_OrlzmDJT",
        "outputId": "eed6cfb7-0d26-4b3f-c2bb-e6011016b2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading Train dataset\n",
            "[INFO] Loading Val dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create the train data loader\n",
        "print(\"Creating Train data loader\")\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)\n",
        "\n",
        "## Create the validation data loader\n",
        "print(\"Creating Val data loader\")\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA_sf-FbmI_Z",
        "outputId": "5d1f0395-2080-4f87-a443-0668aec7a83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Train data loader\n",
            "Creating Val data loader\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "    train_bar = tqdm(train_loader, total=len(train_loader))\n",
        "\n",
        "    running_results = {\"batch_sizes\": 0,\n",
        "                       \"d_loss\": 0, \"g_loss\": 0,\n",
        "                       \"d_score\": 0, \"g_score\": 0,\n",
        "                    }\n",
        "\n",
        "    netG.train()\n",
        "    netD.train()\n",
        "\n",
        "    ## Iterate over the batch of images\n",
        "    for lr_img, hr_img in train_bar:\n",
        "\n",
        "        batch_size = lr_img.size(0)\n",
        "        running_results[\"batch_sizes\"] += batch_size\n",
        "\n",
        "        hr_img = hr_img.to(DEVICE) # high resolution image\n",
        "        lr_img = lr_img.to(DEVICE) # low resolution image\n",
        "        with torch.no_grad():\n",
        "            sr_img = netG(lr_img) # super resolution image\n",
        "\n",
        "        ## Set the gradients of Discriminator to zero\n",
        "        netD.zero_grad()\n",
        "\n",
        "        ## Formward propagate the HR image and SR image through the discriminator\n",
        "        real_out = netD(hr_img).mean()\n",
        "        fake_out = netD(sr_img).mean()\n",
        "\n",
        "        ## Calculate the discriminator loss\n",
        "        d_loss = 1 - real_out + fake_out\n",
        "\n",
        "        ## Backpropagate the loss\n",
        "        d_loss.backward(retain_graph=True)\n",
        "\n",
        "        ## Update the weights\n",
        "        optimizerD.step()\n",
        "\n",
        "        ## Forward propagate the SR image through the discriminator\n",
        "        with torch.no_grad():\n",
        "            fake_out = netD(sr_img).mean()\n",
        "\n",
        "        ## Set the gradients of the Generator to zero\n",
        "        netG.zero_grad()\n",
        "\n",
        "        ## Forward propagate the LR image through the generator to get the SR image\n",
        "        sr_img = netG(lr_img)\n",
        "\n",
        "        ## Calculate the generator loss\n",
        "        g_loss = generator_criterion(fake_out, sr_img, hr_img)\n",
        "\n",
        "        ## Backpropagate the loss\n",
        "        g_loss.backward()\n",
        "\n",
        "        ## Update the weights\n",
        "        optimizerG.step()\n",
        "\n",
        "        running_results[\"g_loss\"] += g_loss.item() * batch_size\n",
        "        running_results[\"d_loss\"] += d_loss.item() * batch_size\n",
        "        running_results[\"d_score\"] += real_out.item() * batch_size\n",
        "        running_results[\"g_score\"] += fake_out.item() * batch_size\n",
        "\n",
        "        train_bar.set_description(\n",
        "            desc=\"[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f\"\n",
        "            % (\n",
        "                epoch,\n",
        "                NUM_EPOCHS,\n",
        "                running_results[\"d_loss\"] / running_results[\"batch_sizes\"],\n",
        "                running_results[\"g_loss\"] / running_results[\"batch_sizes\"],\n",
        "                running_results[\"d_score\"] / running_results[\"batch_sizes\"],\n",
        "                running_results[\"g_score\"] / running_results[\"batch_sizes\"],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    ## Set the Generator to evaluation mode\n",
        "    netG.eval()\n",
        "\n",
        "    ## Run the validation loop\n",
        "    with torch.no_grad():\n",
        "\n",
        "        ## Progress bar for validation loop\n",
        "        val_bar = tqdm(val_loader, total=len(val_loader))\n",
        "\n",
        "        valing_results = {\n",
        "            \"mse\": 0,\n",
        "            \"ssims\": 0,\n",
        "            \"psnr\": 0,\n",
        "            \"ssim\": 0,\n",
        "            \"batch_sizes\": 0,\n",
        "        }\n",
        "\n",
        "        val_images = []\n",
        "\n",
        "        ## Iterate over the batch of images\n",
        "        for val_lr, val_hr in val_bar:\n",
        "\n",
        "            ## Get the current batch size\n",
        "            batch_size = val_lr.size(0)\n",
        "            valing_results[\"batch_sizes\"] += batch_size\n",
        "\n",
        "            lr = val_lr\n",
        "            hr = val_hr\n",
        "            if torch.cuda.is_available():\n",
        "                lr = lr.cuda()\n",
        "                hr = hr.cuda()\n",
        "\n",
        "            ## Forward propagate the LR image through the generator to get the SR image\n",
        "            sr = netG(lr)\n",
        "\n",
        "            ## Calculate All the metrics\n",
        "            ## Calculate and store the MSE\n",
        "            batch_mse = ((sr - hr) ** 2).data.mean()\n",
        "            valing_results[\"mse\"] += batch_mse * batch_size\n",
        "\n",
        "            ## Calculate and store the SSIMs\n",
        "            batch_ssim = ssim(sr, hr).item()\n",
        "            valing_results[\"ssims\"] += batch_ssim * batch_size\n",
        "\n",
        "            ## Calculate and store the PSNR\n",
        "            valing_results[\"psnr\"] = 10 * math.log10( (hr.max() ** 2) / (valing_results[\"mse\"] / valing_results[\"batch_sizes\"]))\n",
        "\n",
        "            ## Calculate and store the SSIM\n",
        "            valing_results[\"ssim\"] = (valing_results[\"ssims\"] / valing_results[\"batch_sizes\"])\n",
        "\n",
        "            ## Update the progress bar and print the results\n",
        "            val_bar.set_description(\n",
        "                desc=\"[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f\"\n",
        "                % (valing_results[\"psnr\"], valing_results[\"ssim\"])\n",
        "            )\n",
        "\n",
        "    netG.train()\n",
        "    netD.train()\n",
        "\n",
        "    ## Save the Generator model\n",
        "    torch.save({\"model\": netG.state_dict()},\n",
        "        f\"netG_{UPSCALE_FACTOR}x_epoch{epoch}.pth.tar\")\n",
        "\n",
        "    ## Save the Discriminator model\n",
        "    torch.save({\"model\": netD.state_dict()},\n",
        "        f\"netD_{UPSCALE_FACTOR}x_epoch{epoch}.pth.tar\")\n",
        "\n",
        "    ## Store the losses and scores for the current epoch\n",
        "    results[\"d_loss\"].append(running_results[\"d_loss\"] / running_results[\"batch_sizes\"])\n",
        "    results[\"g_loss\"].append(running_results[\"g_loss\"] / running_results[\"batch_sizes\"])\n",
        "    results[\"d_score\"].append(running_results[\"d_score\"] / running_results[\"batch_sizes\"])\n",
        "    results[\"g_score\"].append(running_results[\"g_score\"] / running_results[\"batch_sizes\"])\n",
        "    results[\"psnr\"].append(valing_results[\"psnr\"])\n",
        "    results[\"ssim\"].append(valing_results[\"ssim\"])\n",
        "\n",
        "    print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv64vjh_r6cy",
        "outputId": "0831ac52-4f8c-4e8a-eb80-981a9d86d55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1/8] Loss_D: 0.9969 Loss_G: 0.0041 D(x): 0.9992 D(G(z)): 0.9960: 100%|██████████| 2250/2250 [15:22<00:00,  2.44it/s]\n",
            "[converting LR images to SR images] PSNR: 34.5777 dB SSIM: 0.9340: 100%|██████████| 761/761 [00:46<00:00, 16.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d_loss': [0.9969477570586734], 'g_loss': [0.004061847448400739], 'd_score': [0.9992202426989873], 'g_score': [0.9960130303783549], 'psnr': [34.57768020777527], 'ssim': [0.9340427720750365]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2/8] Loss_D: 1.0000 Loss_G: 0.0015 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 2250/2250 [15:16<00:00,  2.45it/s]\n",
            "[converting LR images to SR images] PSNR: 36.4357 dB SSIM: 0.9450: 100%|██████████| 761/761 [00:44<00:00, 16.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d_loss': [0.9969477570586734, 1.0], 'g_loss': [0.004061847448400739, 0.0015366882274910393], 'd_score': [0.9992202426989873, 1.0], 'g_score': [0.9960130303783549, 1.0], 'psnr': [34.57768020777527, 36.43571701741517], 'ssim': [0.9340427720750365, 0.9449760649113398]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[3/8] Loss_D: 1.0000 Loss_G: 0.0014 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 2250/2250 [15:15<00:00,  2.46it/s]\n",
            "[converting LR images to SR images] PSNR: 38.7566 dB SSIM: 0.9503: 100%|██████████| 761/761 [00:44<00:00, 16.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d_loss': [0.9969477570586734, 1.0, 1.0], 'g_loss': [0.004061847448400739, 0.0015366882274910393, 0.0013853977413899783], 'd_score': [0.9992202426989873, 1.0, 1.0], 'g_score': [0.9960130303783549, 1.0, 1.0], 'psnr': [34.57768020777527, 36.43571701741517, 38.75657330482375], 'ssim': [0.9340427720750365, 0.9449760649113398, 0.9503182663084173]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[4/8] Loss_D: 1.0000 Loss_G: 0.0013 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 2250/2250 [15:19<00:00,  2.45it/s]\n",
            "[converting LR images to SR images] PSNR: 39.2482 dB SSIM: 0.9560: 100%|██████████| 761/761 [00:44<00:00, 16.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d_loss': [0.9969477570586734, 1.0, 1.0, 1.0], 'g_loss': [0.004061847448400739, 0.0015366882274910393, 0.0013853977413899783, 0.0013097206223497374], 'd_score': [0.9992202426989873, 1.0, 1.0, 1.0], 'g_score': [0.9960130303783549, 1.0, 1.0, 1.0], 'psnr': [34.57768020777527, 36.43571701741517, 38.75657330482375, 39.2481752884387], 'ssim': [0.9340427720750365, 0.9449760649113398, 0.9503182663084173, 0.9559727827290198]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[5/8] Loss_D: 1.0000 Loss_G: 0.0013 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 2250/2250 [15:17<00:00,  2.45it/s]\n",
            "[converting LR images to SR images] PSNR: 39.5467 dB SSIM: 0.9571: 100%|██████████| 761/761 [00:44<00:00, 16.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d_loss': [0.9969477570586734, 1.0, 1.0, 1.0, 1.0], 'g_loss': [0.004061847448400739, 0.0015366882274910393, 0.0013853977413899783, 0.0013097206223497374, 0.0012697992790231688], 'd_score': [0.9992202426989873, 1.0, 1.0, 1.0, 1.0], 'g_score': [0.9960130303783549, 1.0, 1.0, 1.0, 1.0], 'psnr': [34.57768020777527, 36.43571701741517, 38.75657330482375, 39.2481752884387, 39.54670890577556], 'ssim': [0.9340427720750365, 0.9449760649113398, 0.9503182663084173, 0.9559727827290198, 0.9571178509277364]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[6/8] Loss_D: 1.0000 Loss_G: 0.0012 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 2250/2250 [15:22<00:00,  2.44it/s]\n",
            "[converting LR images to SR images] PSNR: 40.3222 dB SSIM: 0.9580: 100%|██████████| 761/761 [00:44<00:00, 16.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d_loss': [0.9969477570586734, 1.0, 1.0, 1.0, 1.0, 1.0], 'g_loss': [0.004061847448400739, 0.0015366882274910393, 0.0013853977413899783, 0.0013097206223497374, 0.0012697992790231688, 0.001243539672681234], 'd_score': [0.9992202426989873, 1.0, 1.0, 1.0, 1.0, 1.0], 'g_score': [0.9960130303783549, 1.0, 1.0, 1.0, 1.0, 1.0], 'psnr': [34.57768020777527, 36.43571701741517, 38.75657330482375, 39.2481752884387, 39.54670890577556, 40.32220822584583], 'ssim': [0.9340427720750365, 0.9449760649113398, 0.9503182663084173, 0.9559727827290198, 0.9571178509277364, 0.9580364776346906]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[7/8] Loss_D: 1.0000 Loss_G: 0.0012 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 2250/2250 [15:17<00:00,  2.45it/s]\n",
            "[converting LR images to SR images] PSNR: 40.1413 dB SSIM: 0.9579: 100%|██████████| 761/761 [00:44<00:00, 16.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d_loss': [0.9969477570586734, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'g_loss': [0.004061847448400739, 0.0015366882274910393, 0.0013853977413899783, 0.0013097206223497374, 0.0012697992790231688, 0.001243539672681234, 0.0012359368288372126], 'd_score': [0.9992202426989873, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'g_score': [0.9960130303783549, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'psnr': [34.57768020777527, 36.43571701741517, 38.75657330482375, 39.2481752884387, 39.54670890577556, 40.32220822584583, 40.14125488547145], 'ssim': [0.9340427720750365, 0.9449760649113398, 0.9503182663084173, 0.9559727827290198, 0.9571178509277364, 0.9580364776346906, 0.9579243424217585]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[8/8] Loss_D: 1.0000 Loss_G: 0.0012 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 2250/2250 [15:20<00:00,  2.44it/s]\n",
            "[converting LR images to SR images] PSNR: 39.1469 dB SSIM: 0.9577: 100%|██████████| 761/761 [00:45<00:00, 16.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d_loss': [0.9969477570586734, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'g_loss': [0.004061847448400739, 0.0015366882274910393, 0.0013853977413899783, 0.0013097206223497374, 0.0012697992790231688, 0.001243539672681234, 0.0012359368288372126, 0.0012128635379227086], 'd_score': [0.9992202426989873, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'g_score': [0.9960130303783549, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'psnr': [34.57768020777527, 36.43571701741517, 38.75657330482375, 39.2481752884387, 39.54670890577556, 40.32220822584583, 40.14125488547145, 39.14688253257503], 'ssim': [0.9340427720750365, 0.9449760649113398, 0.9503182663084173, 0.9559727827290198, 0.9571178509277364, 0.9580364776346906, 0.9579243424217585, 0.9577492126184131]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing"
      ],
      "metadata": {
        "id": "9LG2cRznc6gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "import sys\n",
        "\n",
        "## Add the scripts folder to the path\n",
        "sys.path.insert(0, '../scripts/')\n",
        "from model_architecture import Generator\n",
        "\n",
        "## Set the seed for reproducibility\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.cuda.manual_seed_all(42)"
      ],
      "metadata": {
        "id": "QJDfcmgThdFD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Set the device\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "## Load the model\n",
        "model = Generator(upscale_factor=4).to(DEVICE)\n",
        "\n",
        "## Load the model weights state dict\n",
        "state_dict = torch.load('/content/netG_4x_epoch8.pth.tar', map_location=torch.device(DEVICE))\n",
        "\n",
        "## Load the model from state dict\n",
        "model.load_state_dict(state_dict[\"model\"], )\n",
        "\n",
        "## Set the model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLpWoCmBhoKx",
        "outputId": "fb29a497-2b28-43d3-cd99-ce0e9de4263f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (initial): ConvBlock(\n",
              "    (cnn): SeperableConv2d(\n",
              "      (depthwise): Conv2d(3, 3, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=3)\n",
              "      (pointwise): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (bn): Identity()\n",
              "    (act): PReLU(num_parameters=64)\n",
              "  )\n",
              "  (residual): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (3): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (4): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (5): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (6): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (7): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (8): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (9): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (10): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (11): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (12): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (13): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (14): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "    (15): ResidualBlock(\n",
              "      (block1): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "      (block2): ConvBlock(\n",
              "        (cnn): SeperableConv2d(\n",
              "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        )\n",
              "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act): PReLU(num_parameters=64)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (convblock): ConvBlock(\n",
              "    (cnn): SeperableConv2d(\n",
              "      (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "      (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act): PReLU(num_parameters=64)\n",
              "  )\n",
              "  (upsampler): Sequential(\n",
              "    (0): UpsampleBlock(\n",
              "      (conv): SeperableConv2d(\n",
              "        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
              "        (pointwise): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (ps): PixelShuffle(upscale_factor=2)\n",
              "      (act): PReLU(num_parameters=64)\n",
              "    )\n",
              "    (1): UpsampleBlock(\n",
              "      (conv): SeperableConv2d(\n",
              "        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
              "        (pointwise): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (ps): PixelShuffle(upscale_factor=2)\n",
              "      (act): PReLU(num_parameters=64)\n",
              "    )\n",
              "  )\n",
              "  (final_conv): SeperableConv2d(\n",
              "    (depthwise): Conv2d(64, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=64)\n",
              "    (pointwise): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image\n",
        "hr_image = Image.open('/content/sample_hr_input.png').convert('RGB')\n",
        "\n",
        "## Create the LR image transformer by downsampling the HR image and applying bicubic interpolation\n",
        "lr_scale = transforms.Resize((256,256), interpolation=Image.BICUBIC)\n",
        "\n",
        "## Create the restored HR image tranformer (simple classical method) by upsampling the LR image and applying bicubic interpolation\n",
        "hr_scale = transforms.Resize((1024,1024), interpolation=Image.BICUBIC)\n",
        "\n",
        "## Create the LR Image from the original HR Image using the LR Image transformer\n",
        "lr_image = lr_scale(hr_image)\n",
        "lr_image.save(\"/content/sample_lr_input.png\")\n",
        "\n",
        "## Create the restored HR Image from the LR Image using the classical method of restored HR Image transforms\n",
        "hr_restore_img = hr_scale(lr_image)\n",
        "\n",
        "## Convert the LR Image to a tensor\n",
        "lr_image = to_tensor(lr_image)\n",
        "\n",
        "# Move the image and model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    lr_image = lr_image.cuda()\n",
        "\n",
        "## Add a batch dimension to the image\n",
        "lr_image = lr_image.unsqueeze(0)\n",
        "\n",
        "lr_image.shape\n",
        "\n",
        "# Perform model inference\n",
        "with torch.no_grad():\n",
        "    output = model(lr_image)"
      ],
      "metadata": {
        "id": "rSNu-hkihvwa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Remove the batch dimension\n",
        "out = output.squeeze(0)\n",
        "\n",
        "## Transforms for displaying the images\n",
        "display_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "])\n",
        "\n",
        "## Transform the output image\n",
        "out = display_transform(out)\n",
        "\n",
        "## Save the output image\n",
        "out.save(\"/content/sample_sr_output.png\")"
      ],
      "metadata": {
        "id": "pXH-SLx3iCVR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNicWPAbr9y7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}